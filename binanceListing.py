import requests
import time
import aiohttp
import asyncio
from datetime import datetime
from bs4 import BeautifulSoup
import json
import re

from config import WEBHOOK_URL, PROXY_URL, USE_PROXY, ALWAYS_NOTIFY, API_URL, MONITOR_INTERVAL

# HTML URL
api_url = API_URL

# ç›‘æ§å‘¨æœŸï¼Œå•ä½ï¼šç§’
monitor_interval = MONITOR_INTERVAL  # æ¯éš” 60 ç§’æŸ¥è¯¢ä¸€æ¬¡

# è®°å½•å·²ç»å¤„ç†è¿‡çš„æ–‡ç« IDï¼Œé¿å…é‡å¤æé†’
processed_article_ids = set()

# æ·»åŠ ä¸€ä¸ªæ–°çš„å˜é‡æ¥å­˜å‚¨ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
last_article_ids = set()

async def send_message_async(message_content):
    """å‘é€æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡æœºå™¨äºº"""
    headers = {'Content-Type': 'application/json'}
    
    payload = {
        "msgtype": "text",
        "text": {
            "content": message_content
        }
    }
    
    proxy = PROXY_URL if USE_PROXY else None
    async with aiohttp.ClientSession() as session:
        async with session.post(WEBHOOK_URL, json=payload, headers=headers, proxy=proxy) as response:
            # print response
            # print(response)

            if response.status == 200:
                log_with_time("Message sent successfully!")
            else:
                log_with_time(f"Failed to send message: {response.status}")

# æ‰“å°æ—¶åŠ ä¸Šæ—¶é—´æˆ³
def log_with_time(message):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ¶ˆæ¯"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{current_time}] {message}")

# åˆå§‹åŒ–æ—¶è·å–ç°æœ‰çš„æ–‡ç« ID
def initialize_processed_articles():
    """åˆå§‹åŒ–æ—¶è·å–å½“å‰æ‰€æœ‰æ–‡ç« IDï¼Œé¿å…å¯åŠ¨æ—¶æé†’å·²æœ‰æ–‡ç« """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(api_url, headers=headers)
        # print(response.text)
        print(response.text)

        soup = BeautifulSoup(response.text, 'html.parser')
        titles = soup.find_all('h1 + div a')

        for title in titles:
            title_text = title.get_text().strip()
            content = title.find_next('div')
            content_text = content.get_text().strip() if content else ""
            announcement_id = hash(title_text + content_text)
            processed_article_ids.add(announcement_id)

        log_with_time("Initialization complete. Monitoring new articles...")
    
    except Exception as e:
        log_with_time(f"Error during initialization: {e}")

def get_emoji_for_type(title):
    """æ ¹æ®å…¬å‘Šæ ‡é¢˜è¿”å›ç›¸åº”çš„ emoji"""
    if "Introducing" in title or "ä¸Šçº¿" in title:
        return "ğŸš€"  # æ–°å¸ç§ä¸Šçº¿
    elif "Launchpool" in title:
        return "ğŸŒ±"  # LaunchPool
    elif "Futures" in title:
        return "ğŸ“ˆ"  # åˆçº¦
    elif "Options" in title:
        return "ğŸ“Š"  # æœŸè´§
    elif "Margin" in title:
        return "ğŸ’¹"  # Margin
    else:
        return "â„¹ï¸"  # å…¶ä»–

def build_listing_message(title, release_date, link):
    """æ„å»ºæ–°å¸ç§ä¸Šçº¿å…¬å‘Šçš„æ¨é€æ¶ˆæ¯"""
    emoji = get_emoji_for_type(title)  # è·å–å¯¹åº”çš„ emoji
    return (
        f"{emoji} æ–°å¸ç§ä¸Šçº¿å…¬å‘Š ğŸ“¢\n"
        f"æ ‡é¢˜: {title}\n"
        f"æ—¶é—´: {release_date}\n"
        f"é“¾æ¥: {link if link else 'æ— é“¾æ¥'}"
    )

# æ£€æŸ¥æ˜¯å¦æœ‰åŒ…å«æ–°å¾—å¸ç§ä¸Šçº¿å…¬å‘Š
async def check_for_new_listing_announcements():
    """æ£€æŸ¥ç½‘é¡µä¸­æ˜¯å¦æœ‰æ–°çš„å¸ç§ä¸Šçº¿å…¬å‘Šå¹¶å‘é€é€šçŸ¥"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(api_url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ‰¾åˆ°æ‰€æœ‰çš„ h1 æ ‡é¢˜
        titles = soup.find_all('h1 + div a')

        for title in titles:
            title_text = title.get_text().strip()
            # è·å–æ ‡é¢˜åé¢çš„ div å†…å®¹
            content = title.find_next('div')
            content_text = content.get_text().strip() if content else ""
            
            # ç”Ÿæˆå”¯ä¸€IDï¼ˆä½¿ç”¨æ ‡é¢˜å’Œå†…å®¹çš„ç»„åˆï¼‰
            announcement_id = hash(title_text + content_text)
            
            if announcement_id not in processed_article_ids:
                message = build_listing_message(title_text, datetime.now().strftime('%Y-%m-%d %H:%M:%S'), title['href'])
                log_with_time(message)
                await send_message_async(message)
                processed_article_ids.add(announcement_id)

    except Exception as e:
        log_with_time(f"Error fetching or processing data: {e}")

# ä¿®æ”¹ monitor å‡½æ•°ä¸ºå¼‚æ­¥å‡½æ•°
async def monitor():
    """å¯åŠ¨ç›‘æ§ç¨‹åº"""
    global last_article_ids
    
    log_with_time("Starting monitor...")
    
    # é¦–æ¬¡è¿è¡Œï¼Œè·å–å½“å‰æ–‡ç« åˆ—è¡¨
    initial_articles = save_and_parse_html_content()

    if initial_articles:
        last_article_ids = {article['id'] for article in initial_articles}
        log_with_time(f"Initialized with {len(last_article_ids)} articles")
        
        # å¦‚æœè®¾ç½®äº†ALWAYS_NOTIFYï¼Œåˆ™å‘é€åˆå§‹æ–‡ç« é€šçŸ¥
        if ALWAYS_NOTIFY:
            for article in initial_articles[:5]:  # åªå‘é€æœ€æ–°çš„5æ¡
                filtered_title = article['title'].replace('Binance', '')
                message = (
                    f"ğŸ“¢ Initial Listing Alert ğŸ“¢\n"
                    f"Title: {filtered_title}\n"
                    f"Time: {article['release_date']}\n"
                    f"Link: {article['link']}"
                )
                await send_message_async(message)

    while True:
        try:
            log_with_time("Checking for new articles...")
            articles = save_and_parse_html_content()
            
            if not articles:
                log_with_time("No articles found in this check")
                await asyncio.sleep(monitor_interval)
                continue
                
            # è·å–å½“å‰æ–‡ç« IDé›†åˆ
            current_article_ids = {article['id'] for article in articles}
            
            # æ‰¾å‡ºæ–°å¢çš„æ–‡ç« ID
            new_article_ids = current_article_ids - last_article_ids
            
            if new_article_ids:
                log_with_time(f"Found {len(new_article_ids)} new articles")
                # è·å–æ–°æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯å¹¶å‘é€é€šçŸ¥
                for article in articles:
                    if article['id'] in new_article_ids:
                        link = article.get('link', 'æ— é“¾æ¥')
                        message = build_listing_message(article['title'], article['release_date'], link)
                        log_with_time(f"Sending notification for article {article['id']}")
                        await send_message_async(message)
            else:
                log_with_time("No new articles found")
            
            # æ›´æ–°ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
            last_article_ids = current_article_ids
            
        except Exception as e:
            log_with_time(f"Error in monitor loop: {e}")
            # å‘é€é”™è¯¯é€šçŸ¥
            await send_message_async(f"âŒ Monitor Error: {str(e)}")
        
        log_with_time(f"Waiting for {monitor_interval} seconds before next check...")
        await asyncio.sleep(monitor_interval)

async def save_html_content():
    """ä¿å­˜HTMLå†…å®¹åˆ°æœ¬åœ°æ–‡ä»¶"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        timeout = aiohttp.ClientTimeout(total=10)  # 10ç§’è¶…æ—¶
        
        log_with_time("Starting request...")
        async with aiohttp.ClientSession(timeout=timeout) as session:
            log_with_time("Sending request to Binance...")
            async with session.get(api_url, headers=headers, proxy=PROXY_URL if USE_PROXY else None) as response:
                log_with_time(f"Response status: {response.status}")
                html = await response.text()
                log_with_time(f"Received content length: {len(html)}")
                
                # ä¿å­˜åŸå§‹HTML
                with open('binance_listing.html', 'w', encoding='utf-8') as f:
                    f.write(html)
                log_with_time("Raw HTML saved to binance_listing.html")
                
                # ä¿å­˜æ ¼å¼åŒ–åçš„HTMLï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
                soup = BeautifulSoup(html, 'html.parser')
                pretty_html = soup.prettify()
                with open('binance_listing_formatted.html', 'w', encoding='utf-8') as f:
                    f.write(pretty_html)
                log_with_time("Formatted HTML saved to binance_listing_formatted.html")
                
                return html

    except aiohttp.ClientError as e:
        log_with_time(f"Network error: {e}")
    except asyncio.TimeoutError:
        log_with_time("Request timed out")
    except Exception as e:
        log_with_time(f"Error saving HTML content: {e}")
    return None

# æˆ–è€…ä½¿ç”¨åŒæ­¥æ–¹å¼å°è¯•
def save_html_content_sync():
    """ä½¿ç”¨åŒæ­¥æ–¹å¼ä¿å­˜HTMLå†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        log_with_time("Starting request...")
        response = requests.get(api_url, headers=headers, timeout=10, proxies={'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None)
        log_with_time(f"Response status: {response.status_code}")
        
        html = response.text
        log_with_time(f"Received content length: {len(html)}")
        
        # ä¿å­˜åŸå§‹HTML
        with open('binance_listing.html', 'w', encoding='utf-8') as f:
            f.write(html)
        log_with_time("Raw HTML saved to binance_listing.html")
        
        # ä¿å­˜æ ¼å¼åŒ–åçš„HTML
        soup = BeautifulSoup(html, 'html.parser')
        pretty_html = soup.prettify()
        with open('binance_listing_formatted.html', 'w', encoding='utf-8') as f:
            f.write(pretty_html)
        log_with_time("Formatted HTML saved to binance_listing_formatted.html")
        
        return html
        
    except requests.RequestException as e:
        log_with_time(f"Request error: {e}")
    except Exception as e:
        log_with_time(f"Error saving HTML content: {e}")
    return None

def parse_listing_data(html_content):
    """ä»HTMLå†…å®¹ä¸­è§£æå‡ºæ–°å¸ä¸Šçº¿ä¿¡æ¯"""
    try:
        # æŸ¥æ‰¾åŒ…å«ç›®æ ‡æ•°æ®çš„scriptæ ‡ç­¾
        pattern = r'<script id="__APP_DATA" type="application/json".*?>(.*?)</script>'
        script_content = re.search(pattern, html_content, re.DOTALL)
        
        if not script_content:
            log_with_time("No APP_DATA script found")
            return None
            
        # è§£æJSONæ•°æ®
        json_data = json.loads(script_content.group(1))
        
        # éå† dataByRouteId æŸ¥æ‰¾ç›®æ ‡ catalog
        route_data = json_data['appState']['loader']['dataByRouteId']
        target_catalog = None
        
        for key in route_data:
            data = route_data[key]
            if 'catalogDetail' in data and data['catalogDetail'].get('catalogName') == "New Cryptocurrency Listing":
                target_catalog = data['catalogDetail']
                break
                
        if not target_catalog:
            log_with_time("No New Cryptocurrency Listing catalog found")
            return None
            
        # è§£ææ–‡ç« åˆ—è¡¨
        articles = target_catalog['articles']
        log_with_time(f"Found {len(articles)} articles")
        
        # æ ¼å¼åŒ–æ–‡ç« ä¿¡æ¯
        formatted_articles = []
        for article in articles:
            formatted_article = {
                'id': article['id'],
                'code': article['code'],
                'title': article['title'],
                'release_date': datetime.fromtimestamp(article['releaseDate']/1000).strftime('%Y-%m-%d %H:%M:%S'),
                'link': build_article_link(article['title'], article['code'])
            }
            formatted_articles.append(formatted_article)
            
        return formatted_articles

    except json.JSONDecodeError as e:
        log_with_time(f"JSON parsing error: {e}")
    except Exception as e:
        log_with_time(f"Error parsing listing data: {e}")
    return None

def save_and_parse_html_content():
    """ä¿å­˜HTMLå†…å®¹å¹¶è§£ææ•°æ®"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        log_with_time("Starting request...")
        response = requests.get(api_url, headers=headers, timeout=10, 
                              proxies={'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None)
        log_with_time(f"Response status: {response.status_code}")
        
        html = response.text
        log_with_time(f"Received content length: {len(html)}")
        
        # ä¿å­˜åŸå§‹HTML
        with open('binance_listing.html', 'w', encoding='utf-8') as f:
            f.write(html)
        log_with_time("Raw HTML saved to binance_listing.html")
        
        # è§£ææ•°æ®
        articles = parse_listing_data(html)
        if articles:
            # ä¿å­˜è§£æåçš„æ•°æ®
            with open('binance_listing_data.json', 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            log_with_time("Parsed data saved to binance_listing_data.json")
            
            # æ‰“å°æœ€æ–°çš„5æ¡å…¬å‘Š
            log_with_time("\nLatest 5 announcements:")
            for article in articles[:5]:
                log_with_time(f"Title: {article['title']}")
                log_with_time(f"Release Date: {article['release_date']}")
                log_with_time("-" * 50)
        
        return articles
        
    except requests.RequestException as e:
        log_with_time(f"Request error: {e}")
    except Exception as e:
        log_with_time(f"Error in save_and_parse_html_content: {e}")
    return None

def build_article_link(title, code):
    """æ„å»ºæ–‡ç« é“¾æ¥"""
    base_url = "https://www.binance.com/en/support/announcement/"
    # å°†æ ‡é¢˜ä¸­çš„ç©ºæ ¼æ›¿æ¢ä¸º "-"ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå°å†™
    formatted_title = title.replace(" ", "-").lower()
    return f"{base_url}{formatted_title}-{code}"

# ä¿®æ”¹ main éƒ¨åˆ†
if __name__ == "__main__":
    # æ­£å¸¸ç›‘æ§æ¨¡å¼
    asyncio.run(monitor())
