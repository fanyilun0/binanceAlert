import requests
import time
import aiohttp
import asyncio
import random
from datetime import datetime
from bs4 import BeautifulSoup
import json
import re

from config import COOKIE, WEBHOOK_URL, PROXY_URL, USE_PROXY, ALWAYS_NOTIFY, API_URL, MONITOR_INTERVAL

# HTML URL
api_url = API_URL

# ç›‘æ§å‘¨æœŸï¼Œå•ä½ï¼šç§’
monitor_interval = MONITOR_INTERVAL  # æ¯éš” 60 ç§’æŸ¥è¯¢ä¸€æ¬¡

# è®°å½•å·²ç»å¤„ç†è¿‡çš„æ–‡ç« IDï¼Œé¿å…é‡å¤æé†’
processed_article_ids = set()

# æ·»åŠ ä¸€ä¸ªæ–°çš„å˜é‡æ¥å­˜å‚¨ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
last_article_ids = set()

async def send_message_async(message_content):
    """å‘é€æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡æœºå™¨äºº"""
    headers = {'Content-Type': 'application/json'}
    
    payload = {
        "msgtype": "text",
        "text": {
            "content": message_content
        }
    }
    
    proxy = PROXY_URL if USE_PROXY else None
    async with aiohttp.ClientSession() as session:
        async with session.post(WEBHOOK_URL, json=payload, headers=headers, proxy=proxy) as response:
            # print response
            # print(response)

            if response.status == 200:
                log_with_time("Message sent successfully!")
            else:
                log_with_time(f"Failed to send message: {response.status}")

# æ‰“å°æ—¶åŠ ä¸Šæ—¶é—´æˆ³
def log_with_time(message):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ¶ˆæ¯"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{current_time}] {message}")

def get_random_headers():   
    # æ·»åŠ User-Agentæ± 
    USER_AGENTS = [
        # Chrome
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        # Firefox
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
        # Safari
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        # Edge
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
        # Mobile
        "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1.2 Mobile/15E148 Safari/604.1",
        "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"
    ]
    """ç”Ÿæˆéšæœºçš„è¯·æ±‚å¤´"""
    return {
        'authority': 'www.binance.com',
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-encoding': 'gzip, deflate, br, zstd',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7',
        'cache-control': 'max-age=0',
        'User-Agent': random.choice(USER_AGENTS),
        'cookie': COOKIE,
        'referer': 'https://www.binance.com/en/support/announcement/new-cryptocurrency-listing?c=48&navId=48&hl=en',
        'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1'
    }

def get_emoji_for_type(title):
    """æ ¹æ®å…¬å‘Šæ ‡é¢˜è¿”å›ç›¸åº”çš„ emoji"""
    if "Introducing" in title or "ä¸Šçº¿" in title:
        return "ğŸš€"  # æ–°å¸ç§ä¸Šçº¿
    elif "Launchpool" in title:
        return "ğŸŒ±"  # LaunchPool
    elif "Futures" in title:
        return "ğŸ“ˆ"  # åˆçº¦
    elif "Options" in title:
        return "ğŸ“Š"  # æœŸè´§
    elif "Margin" in title:
        return "ğŸ’¹"  # Margin
    else:
        return "â„¹ï¸"  # å…¶ä»–

def build_listing_message(title, release_date, link):
    """æ„å»ºæ–°å¸ç§ä¸Šçº¿å…¬å‘Šçš„æ¨é€æ¶ˆæ¯"""
    emoji = get_emoji_for_type(title)  # è·å–å¯¹åº”çš„ emoji
    return (
        f"{emoji} æ–°å¸ç§ä¸Šçº¿å…¬å‘Š ğŸ“¢\n"
        f"æ ‡é¢˜: {title}\n"
        f"æ—¶é—´: {release_date}\n"
        f"é“¾æ¥: {link if link else 'æ— é“¾æ¥'}"
    )


def parse_listing_data(html_content):
    """ä»HTMLå†…å®¹ä¸­è§£æå‡ºæ–°å¸ä¸Šçº¿ä¿¡æ¯"""
    try:
        # æŸ¥æ‰¾åŒ…å«ç›®æ ‡æ•°æ®çš„scriptæ ‡ç­¾
        pattern = r'<script id="__APP_DATA" type="application/json".*?>(.*?)</script>'
        script_content = re.search(pattern, html_content, re.DOTALL)

        # Debug
        # log_with_time(f"script_content: {script_content}")

        if not script_content:
            log_with_time("No APP_DATA script found")
            return None
            
        # è§£æJSONæ•°æ®
        json_data = json.loads(script_content.group(1))
        
        # ç›´æ¥æŸ¥æ‰¾åŒ…å« catalogDetail çš„è·¯ç”±
        route_data = None
        for route_content in json_data['appState']['loader']['dataByRouteId'].values():
            if 'catalogDetail' in route_content:
                route_data = route_content
                break
                
        if not route_data or 'catalogDetail' not in route_data:
            log_with_time("No route with catalogDetail found")
            return None
            
        catalog_detail = route_data['catalogDetail']

        log_with_time(f"catalog_detail: {catalog_detail}")

        if catalog_detail['catalogName'] != 'New Cryptocurrency Listing':
            log_with_time(f"Unexpected catalog name: {catalog_detail['catalogName']}")
            return None
            
        articles = catalog_detail['articles']
        log_with_time(f"Found {len(articles)} articles")
        
        formatted_articles = []
        for article in articles:
            formatted_article = {
                'id': article['id'],
                'code': article['code'],
                'title': article['title'],
                'release_date': datetime.fromtimestamp(article['releaseDate']/1000).strftime('%Y-%m-%d %H:%M:%S'),
                'link': build_article_link(article['title'], article['code'])
            }
            formatted_articles.append(formatted_article)
            
        return formatted_articles

    except json.JSONDecodeError as e:
        log_with_time(f"JSON parsing error: {e}")
    except Exception as e:
        log_with_time(f"Error parsing listing data: {e}")
        import traceback
        log_with_time(f"Detailed error: {traceback.format_exc()}")
    return None

def save_and_parse_html_content():
    """ä¿å­˜HTMLå†…å®¹å¹¶è§£ææ•°æ®"""
    try:
        headers = get_random_headers()
        
        log_with_time("Starting request...")
        # log_with_time(f"Using Proxy: {PROXY_URL}")

        response = requests.get(api_url, headers=headers, timeout=10, 
                              proxies={'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None)
        log_with_time(f"Response status: {response.status_code}")
        
        html = response.text
        log_with_time(f"Received content length: {len(html)}")
        
        # ä¿å­˜åŸå§‹HTML
        with open('binance_listing.html', 'w', encoding='utf-8') as f:
            f.write(html)
        log_with_time("Raw HTML saved to binance_listing.html")
        
        # è§£ææ•°æ®
        articles = parse_listing_data(html)
        if articles:
            # ä¿å­˜è§£æåçš„æ•°æ®
            with open('binance_listing_data.json', 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            log_with_time("Parsed data saved to binance_listing_data.json")
            
            # æ‰“å°æœ€æ–°çš„5æ¡å…¬å‘Š
            log_with_time("\nLatest 5 announcements:")
            for article in articles[:5]:
                log_with_time(f"Title: {article['title']}")
                log_with_time(f"Release Date: {article['release_date']}")
                log_with_time("-" * 50)
        
        return articles
        
    except requests.RequestException as e:
        log_with_time(f"Request error: {e}")
    except Exception as e:
        log_with_time(f"Error in save_and_parse_html_content: {e}")
    return None

def build_article_link(title, code):
    """æ„å»ºæ–‡ç« é“¾æ¥"""
    base_url = "https://www.binance.com/en/support/announcement/"
    # å°†æ ‡é¢˜ä¸­çš„ç©ºæ ¼æ›¿æ¢ä¸º "-"ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå°å†™
    formatted_title = title.replace(" ", "-").lower()
    return f"{base_url}{formatted_title}-{code}"

# ä¿®æ”¹ monitor å‡½æ•°ä¸ºå¼‚æ­¥å‡½æ•°
async def monitor():
    """å¯åŠ¨ç›‘æ§ç¨‹åº"""
    global last_article_ids
    
    log_with_time("Starting monitor...")
    
    # é¦–æ¬¡è¿è¡Œï¼Œè·å–å½“å‰æ–‡ç« åˆ—è¡¨
    initial_articles = save_and_parse_html_content()

    if initial_articles:
        last_article_ids = {article['id'] for article in initial_articles}
        log_with_time(f"Initialized with {len(last_article_ids)} articles")
        
        # å¦‚æœè®¾ç½®äº†ALWAYS_NOTIFYï¼Œåˆ™å‘é€åˆå§‹æ–‡ç« é€šçŸ¥
        if ALWAYS_NOTIFY:
            for article in initial_articles[:5]:  # åªå‘é€æœ€æ–°çš„5æ¡
                filtered_title = article['title'].replace('Binance', '')
                message = (
                    f"ğŸ“¢ Initial Listing Alert ğŸ“¢\n"
                    f"Title: {filtered_title}\n"
                    f"Time: {article['release_date']}\n"
                    f"Link: {article['link']}"
                )
                await send_message_async(message)

    while True:
        try:
            log_with_time("Checking for new articles...")
            articles = save_and_parse_html_content()
            
            if not articles:
                log_with_time("No articles found in this check")
                await asyncio.sleep(monitor_interval)
                continue
                
            # è·å–å½“å‰æ–‡ç« IDé›†åˆ
            current_article_ids = {article['id'] for article in articles}
            
            # æ‰¾å‡ºæ–°å¢çš„æ–‡ç« ID
            new_article_ids = current_article_ids - last_article_ids
            
            if new_article_ids:
                log_with_time(f"Found {len(new_article_ids)} new articles")
                # è·å–æ–°æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯å¹¶å‘é€é€šçŸ¥
                for article in articles:
                    if article['id'] in new_article_ids:
                        link = article.get('link', 'æ— é“¾æ¥')
                        message = build_listing_message(article['title'], article['release_date'], link)
                        log_with_time(f"Sending notification for article {article['id']}")
                        await send_message_async(message)
            else:
                log_with_time("No new articles found")
            
            # æ›´æ–°ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
            last_article_ids = current_article_ids
            
        except Exception as e:
            log_with_time(f"Error in monitor loop: {e}")
            # å‘é€é”™è¯¯é€šçŸ¥
            await send_message_async(f"âŒ Monitor Error: {str(e)}")
        
        log_with_time(f"Waiting for {monitor_interval} seconds before next check...")
        await asyncio.sleep(monitor_interval)


# ä¿®æ”¹ main éƒ¨åˆ†
if __name__ == "__main__":
    # æ­£å¸¸ç›‘æ§æ¨¡å¼
    asyncio.run(monitor())
