import requests
import time
import aiohttp
import asyncio
from datetime import datetime
import bs4
from bs4 import BeautifulSoup
import json
import re

# Add new configuration constants
WEBHOOK_URL = 'https://qyapi.weixin.qq.com/cgi-bin/webhook/send?'
PROXY_URL = 'http://localhost:7890'
USE_PROXY = True
ALWAYS_NOTIFY = False

# HTML URL
api_url = "https://www.binance.com/en/support/announcement/new-cryptocurrency-listing?c=48&navId=48&hl=en"

# è®°å½•å·²ç»å¤„ç†è¿‡çš„æ–‡ç« IDï¼Œé¿å…é‡å¤æé†’
processed_article_ids = set()

# ç›‘æ§å‘¨æœŸï¼Œå•ä½ï¼šç§’
monitor_interval = 60  # æ¯éš” 60 ç§’æŸ¥è¯¢ä¸€æ¬¡

# æ·»åŠ ä¸€ä¸ªæ–°çš„å˜é‡æ¥å­˜å‚¨ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
last_article_ids = set()

async def send_message_async(message_content):
    """å‘é€æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡æœºå™¨äºº"""
    headers = {'Content-Type': 'application/json'}
    
    payload = {
        "msgtype": "text",
        "text": {
            "content": message_content
        }
    }
    
    proxy = PROXY_URL if USE_PROXY else None
    async with aiohttp.ClientSession() as session:
        async with session.post(WEBHOOK_URL, json=payload, headers=headers, proxy=proxy) as response:
            if response.status == 200:
                log_with_time("Message sent successfully!")
            else:
                log_with_time(f"Failed to send message: {response.status}")

# æ‰“å°æ—¶åŠ ä¸Šæ—¶é—´æˆ³
def log_with_time(message):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ¶ˆæ¯"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{current_time}] {message}")

# åˆå§‹åŒ–æ—¶è·å–ç°æœ‰çš„æ–‡ç« ID
def initialize_processed_articles():
    """åˆå§‹åŒ–æ—¶è·å–å½“å‰æ‰€æœ‰æ–‡ç« IDï¼Œé¿å…å¯åŠ¨æ—¶æé†’å·²æœ‰æ–‡ç« """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(api_url, headers=headers)
        # print(response.text)
        print(response.text)

        soup = BeautifulSoup(response.text, 'html.parser')
        titles = soup.find_all('h1 + div a')
        # print(titles)
        print(len(titles))

        for title in titles:
            title_text = title.get_text().strip()
            content = title.find_next('div')
            content_text = content.get_text().strip() if content else ""
            announcement_id = hash(title_text + content_text)
            processed_article_ids.add(announcement_id)

        log_with_time("Initialization complete. Monitoring new articles...")
    
    except Exception as e:
        log_with_time(f"Error during initialization: {e}")


# æ£€æŸ¥æ˜¯å¦æœ‰åŒ…å«æ–°å¾—å¸ç§ä¸Šçº¿å…¬å‘Š
async def check_for_launchpool_articles():
    """æ£€æŸ¥ç½‘é¡µä¸­æ˜¯å¦æœ‰æ–°çš„å¸ç§ä¸Šçº¿å…¬å‘Š"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(api_url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ‰¾åˆ°æ‰€æœ‰çš„ h1 æ ‡é¢˜
        titles = soup.find_all('h1 + div a')
        # print(titles)
        print(len(titles))

        for title in titles:
            title_text = title.get_text().strip()
            # è·å–æ ‡é¢˜åé¢çš„ div å†…å®¹
            content = title.find_next('div')
            content_text = content.get_text().strip() if content else ""
            
            # ç”Ÿæˆå”¯ä¸€IDï¼ˆä½¿ç”¨æ ‡é¢˜å’Œå†…å®¹çš„ç»„åˆï¼‰
            announcement_id = hash(title_text + content_text)
            
            if announcement_id not in processed_article_ids and "New Cryptocurrency Listing" in title_text:
                message = f"New Listing Found:\nTitle: {title_text}\nContent: {content_text}"
                log_with_time(message)
                await send_message_async(message)
                processed_article_ids.add(announcement_id)

    except Exception as e:
        log_with_time(f"Error fetching or processing data: {e}")

# ä¿®æ”¹ monitor å‡½æ•°ä¸ºå¼‚æ­¥å‡½æ•°
async def monitor():
    """å¯åŠ¨ç›‘æ§ç¨‹åº"""
    global last_article_ids
    
    log_with_time("Starting monitor...")
    
    # é¦–æ¬¡è¿è¡Œï¼Œè·å–å½“å‰æ–‡ç« åˆ—è¡¨
    initial_articles = save_and_parse_html_content()
    if initial_articles:
        last_article_ids = {article['id'] for article in initial_articles}
        log_with_time(f"Initialized with {len(last_article_ids)} articles")

    while True:
        try:
            log_with_time("Checking for new articles...")
            articles = save_and_parse_html_content()
            
            if not articles:
                log_with_time("No articles found in this check")
                await asyncio.sleep(monitor_interval)
                continue
                
            # è·å–å½“å‰æ–‡ç« IDé›†åˆ
            current_article_ids = {article['id'] for article in articles}
            
            # æ‰¾å‡ºæ–°å¢çš„æ–‡ç« ID
            new_article_ids = current_article_ids - last_article_ids
            
            if new_article_ids:
                log_with_time(f"Found {len(new_article_ids)} new articles")
                # è·å–æ–°æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯å¹¶å‘é€é€šçŸ¥
                for article in articles:
                    if article['id'] in new_article_ids:
                        message = (
                            f"ğŸ”¥ New Listing Alert ğŸ”¥\n"
                            f"Title: {article['title']}\n"
                            f"Time: {article['release_date']}\n"
                            f"Link: https://www.binance.com/en/support/announcement/{article['code']}"
                        )
                        log_with_time(f"Sending notification for article {article['id']}")
                        await send_message_async(message)
            else:
                log_with_time("No new articles found")
            
            # æ›´æ–°ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
            last_article_ids = current_article_ids
            
        except Exception as e:
            log_with_time(f"Error in monitor loop: {e}")
        
        log_with_time(f"Waiting for {monitor_interval} seconds before next check...")
        await asyncio.sleep(monitor_interval)

# æ·»åŠ ä¸€ä¸ªè°ƒè¯•å‡½æ•°æ¥æ¨¡æ‹Ÿæ–°æ–‡ç« 
async def test_monitor():
    """æµ‹è¯•ç›‘æ§åŠŸèƒ½"""
    global last_article_ids
    
    # æ¨¡æ‹Ÿç¬¬ä¸€æ¬¡è·å–æ•°æ®
    test_data_1 = [
        {'id': 1, 'title': 'Old Article 1', 'code': 'code1', 'release_date': '2024-01-01'},
        {'id': 2, 'title': 'Old Article 2', 'code': 'code2', 'release_date': '2024-01-02'}
    ]
    
    # æ¨¡æ‹Ÿç¬¬äºŒæ¬¡è·å–æ•°æ®ï¼ˆæ·»åŠ äº†æ–°æ–‡ç« ï¼‰
    test_data_2 = [
        {'id': 1, 'title': 'Old Article 1', 'code': 'code1', 'release_date': '2024-01-01'},
        {'id': 2, 'title': 'Old Article 2', 'code': 'code2', 'release_date': '2024-01-02'},
        {'id': 3, 'title': 'New Article 3', 'code': 'code3', 'release_date': '2024-01-03'}
    ]
    
    # ç¬¬ä¸€æ¬¡è¿è¡Œ
    last_article_ids = {article['id'] for article in test_data_1}
    log_with_time("Test: Initialized with test data")
    
    # ç­‰å¾…ä¸€ç§’
    await asyncio.sleep(1)
    
    # æ¨¡æ‹Ÿå‘ç°æ–°æ–‡ç« 
    current_article_ids = {article['id'] for article in test_data_2}
    new_article_ids = current_article_ids - last_article_ids
    
    if new_article_ids:
        for article in test_data_2:
            if article['id'] in new_article_ids:
                message = (
                    f"ğŸ”¥ Test: New Listing Alert ğŸ”¥\n"
                    f"Title: {article['title']}\n"
                    f"Time: {article['release_date']}\n"
                    f"Link: https://www.binance.com/en/support/announcement/{article['code']}"
                )
                await send_message_async(message)

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ä¸€ä¸ªæµ‹è¯•å‡½æ•°
async def test_webhook():
    """æµ‹è¯•ä¼ä¸šå¾®ä¿¡æœºå™¨äººæ¶ˆæ¯æ¨é€"""
    test_message = "è¿™æ˜¯ä¸€æ¡æµ‹è¯•æ¶ˆæ¯ - " + datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_with_time("å‘é€æµ‹è¯•æ¶ˆæ¯...")
    await send_message_async(test_message)

async def test_article_detection():
    """æµ‹è¯•æ–‡ç« æ£€æµ‹åŠŸèƒ½"""
    # æ¸…ç©ºå·²å¤„ç†æ–‡ç« é›†åˆ
    processed_article_ids.clear()
    
    # è¿è¡Œä¸€æ¬¡æ£€æµ‹
    await check_for_launchpool_articles()
    
    # ç­‰å¾…æ¶ˆæ¯å‘é€å®Œæˆ
    await asyncio.sleep(2)

async def save_html_content():
    """ä¿å­˜HTMLå†…å®¹åˆ°æœ¬åœ°æ–‡ä»¶"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        timeout = aiohttp.ClientTimeout(total=10)  # 10ç§’è¶…æ—¶
        
        log_with_time("Starting request...")
        async with aiohttp.ClientSession(timeout=timeout) as session:
            log_with_time("Sending request to Binance...")
            async with session.get(api_url, headers=headers, proxy=PROXY_URL if USE_PROXY else None) as response:
                log_with_time(f"Response status: {response.status}")
                html = await response.text()
                log_with_time(f"Received content length: {len(html)}")
                
                # ä¿å­˜åŸå§‹HTML
                with open('binance_listing.html', 'w', encoding='utf-8') as f:
                    f.write(html)
                log_with_time("Raw HTML saved to binance_listing.html")
                
                # ä¿å­˜æ ¼å¼åŒ–åçš„HTMLï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
                soup = BeautifulSoup(html, 'html.parser')
                pretty_html = soup.prettify()
                with open('binance_listing_formatted.html', 'w', encoding='utf-8') as f:
                    f.write(pretty_html)
                log_with_time("Formatted HTML saved to binance_listing_formatted.html")
                
                return html

    except aiohttp.ClientError as e:
        log_with_time(f"Network error: {e}")
    except asyncio.TimeoutError:
        log_with_time("Request timed out")
    except Exception as e:
        log_with_time(f"Error saving HTML content: {e}")
    return None

# æˆ–è€…ä½¿ç”¨åŒæ­¥æ–¹å¼å°è¯•
def save_html_content_sync():
    """ä½¿ç”¨åŒæ­¥æ–¹å¼ä¿å­˜HTMLå†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        log_with_time("Starting request...")
        response = requests.get(api_url, headers=headers, timeout=10, proxies={'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None)
        log_with_time(f"Response status: {response.status_code}")
        
        html = response.text
        log_with_time(f"Received content length: {len(html)}")
        
        # ä¿å­˜åŸå§‹HTML
        with open('binance_listing.html', 'w', encoding='utf-8') as f:
            f.write(html)
        log_with_time("Raw HTML saved to binance_listing.html")
        
        # ä¿å­˜æ ¼å¼åŒ–åçš„HTML
        soup = BeautifulSoup(html, 'html.parser')
        pretty_html = soup.prettify()
        with open('binance_listing_formatted.html', 'w', encoding='utf-8') as f:
            f.write(pretty_html)
        log_with_time("Formatted HTML saved to binance_listing_formatted.html")
        
        return html
        
    except requests.RequestException as e:
        log_with_time(f"Request error: {e}")
    except Exception as e:
        log_with_time(f"Error saving HTML content: {e}")
    return None

def parse_listing_data(html_content):
    """ä»HTMLå†…å®¹ä¸­è§£æå‡ºæ–°å¸ä¸Šçº¿ä¿¡æ¯"""
    try:
        # æŸ¥æ‰¾åŒ…å«ç›®æ ‡æ•°æ®çš„scriptæ ‡ç­¾
        pattern = r'<script id="__APP_DATA" type="application/json".*?>(.*?)</script>'
        script_content = re.search(pattern, html_content, re.DOTALL)
        
        if not script_content:
            log_with_time("No APP_DATA script found")
            return None
            
        # è§£æJSONæ•°æ®
        json_data = json.loads(script_content.group(1))
        
        # æŸ¥æ‰¾ç›®æ ‡catalog
        catalogs = json_data['appState']['loader']['dataByRouteId']['d9b2']['catalogs']
        target_catalog = None
        
        for catalog in catalogs:
            if catalog.get('catalogName') == "New Cryptocurrency Listing":
                target_catalog = catalog
                break
                
        if not target_catalog:
            log_with_time("No New Cryptocurrency Listing catalog found")
            return None
            
        # è§£ææ–‡ç« åˆ—è¡¨
        articles = target_catalog['articles']
        log_with_time(f"Found {len(articles)} articles")
        
        # æ ¼å¼åŒ–æ–‡ç« ä¿¡æ¯
        formatted_articles = []
        for article in articles:
            formatted_article = {
                'id': article['id'],
                'code': article['code'],
                'title': article['title'],
                'release_date': datetime.fromtimestamp(article['releaseDate']/1000).strftime('%Y-%m-%d %H:%M:%S')
            }
            formatted_articles.append(formatted_article)
            
        return formatted_articles

    except json.JSONDecodeError as e:
        log_with_time(f"JSON parsing error: {e}")
    except Exception as e:
        log_with_time(f"Error parsing listing data: {e}")
    return None

def save_and_parse_html_content():
    """ä¿å­˜HTMLå†…å®¹å¹¶è§£ææ•°æ®"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        log_with_time("Starting request...")
        response = requests.get(api_url, headers=headers, timeout=10, 
                              proxies={'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None)
        log_with_time(f"Response status: {response.status_code}")
        
        html = response.text
        log_with_time(f"Received content length: {len(html)}")
        
        # ä¿å­˜åŸå§‹HTML
        with open('binance_listing.html', 'w', encoding='utf-8') as f:
            f.write(html)
        log_with_time("Raw HTML saved to binance_listing.html")
        
        # è§£ææ•°æ®
        articles = parse_listing_data(html)
        if articles:
            # ä¿å­˜è§£æåçš„æ•°æ®
            with open('binance_listing_data.json', 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            log_with_time("Parsed data saved to binance_listing_data.json")
            
            # æ‰“å°æœ€æ–°çš„5æ¡å…¬å‘Š
            log_with_time("\nLatest 5 announcements:")
            for article in articles[:5]:
                log_with_time(f"\nTitle: {article['title']}")
                log_with_time(f"Release Date: {article['release_date']}")
                log_with_time(f"ID: {article['id']}")
                log_with_time("-" * 50)
        
        return articles
        
    except requests.RequestException as e:
        log_with_time(f"Request error: {e}")
    except Exception as e:
        log_with_time(f"Error in save_and_parse_html_content: {e}")
    return None

# ä¿®æ”¹ main éƒ¨åˆ†
if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    # asyncio.run(test_monitor())
    
    # æ–‡ç« æ£€æµ‹æµ‹è¯•
    # asyncio.run(test_article_detection())

    # ä¿å­˜HTMLå†…å®¹
    # asyncio.run(save_html_content())

    # æˆ–è€…å°è¯•åŒæ­¥æ–¹å¼
    # save_html_content_sync()

    # articles = save_and_parse_html_content()

    # è¿è¡Œæµ‹è¯•
    # asyncio.run(test_monitor())

    # æ­£å¸¸ç›‘æ§æ¨¡å¼
    asyncio.run(monitor())
